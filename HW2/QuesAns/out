nohup: ignoring input
2021-04-29 21:05:57.446569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
04/29/2021 21:05:58 - INFO - __main__ -    Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Use FP16 precision: False

04/29/2021 21:05:59 - WARNING - datasets.builder -    Using custom data configuration default-d00d4131cb67bf66
0 tables [00:00, ? tables/s]                            loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/boewoei0123/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.5.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/boewoei0123/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.5.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /home/boewoei0123/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /home/boewoei0123/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d
loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /home/boewoei0123/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
04/29/2021 21:06:04 - INFO - __main__ -    Saving tokenizer to ./saved/0429-2105/tokenizer...
tokenizer config file saved in ./saved/0429-2105/tokenizer_config.json
Special tokens file saved in ./saved/0429-2105/special_tokens_map.json
loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /home/boewoei0123/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/boewoei0123/.cache/huggingface/datasets/json/default-d00d4131cb67bf66/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...
Dataset json downloaded and prepared to /home/boewoei0123/.cache/huggingface/datasets/json/default-d00d4131cb67bf66/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.
#0:   0%|          | 0/7 [00:00<?, ?ba/s]
#1:   0%|          | 0/7 [00:00<?, ?ba/s][A

#2:   0%|          | 0/7 [00:00<?, ?ba/s][A[A


#3:   0%|          | 0/7 [00:00<?, ?ba/s][A[A[A#0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.16ba/s]
#1:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.15ba/s][A

#2:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.15ba/s][A[A


#3:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.18ba/s][A[A[A#0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.22ba/s]
#1:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.21ba/s][A

#2:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.22ba/s][A[A


#3:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.24ba/s][A[A[A#0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.27ba/s]
#1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.26ba/s][A

#2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.26ba/s][A[A


#3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.28ba/s][A[A[A#0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.27ba/s]
#1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.25ba/s][A

#2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.26ba/s][A[A


#3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.27ba/s][A[A[A#0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:03<00:01,  1.30ba/s]
#1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:03<00:01,  1.29ba/s][A

#2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:03<00:01,  1.29ba/s][A[A


#3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:03<00:01,  1.30ba/s][A[A[A#0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:04<00:00,  1.32ba/s]#0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.51ba/s]
#1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:04<00:00,  1.31ba/s][A

#2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:04<00:00,  1.31ba/s][A[A#1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.50ba/s]


#3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:04<00:00,  1.32ba/s][A[A[A#2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.50ba/s]


#3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.78ba/s][A[A[A#3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.51ba/s]



04/29/2021 21:06:14 - INFO - __main__ -    
******** Running training ********
04/29/2021 21:06:14 - INFO - __main__ -    Num examples = 43283
04/29/2021 21:06:14 - INFO - __main__ -    Num Epochs = 10
04/29/2021 21:06:14 - INFO - __main__ -    Instantaneous batch size per device = 8
04/29/2021 21:06:14 - INFO - __main__ -    Total train batch size (w/ parallel, distributed & accumulation) = 128
04/29/2021 21:06:14 - INFO - __main__ -    Gradient Accumulation steps = 16
04/29/2021 21:06:14 - INFO - __main__ -    Total optimization steps = 3390
04/29/2021 21:06:14 - INFO - __main__ -    
Epoch 01 / 10
04/29/2021 21:06:50 - INFO - __main__ -    Train | Loss: 5.99319
04/29/2021 21:07:25 - INFO - __main__ -    Train | Loss: 5.69450
04/29/2021 21:08:01 - INFO - __main__ -    Train | Loss: 5.21335
04/29/2021 21:08:38 - INFO - __main__ -    Train | Loss: 4.76921
04/29/2021 21:09:14 - INFO - __main__ -    Train | Loss: 4.29683
04/29/2021 21:09:50 - INFO - __main__ -    Train | Loss: 3.85879
04/29/2021 21:10:26 - INFO - __main__ -    Train | Loss: 3.52164
04/29/2021 21:11:02 - INFO - __main__ -    Train | Loss: 3.24227
04/29/2021 21:11:38 - INFO - __main__ -    Train | Loss: 3.00772
04/29/2021 21:12:14 - INFO - __main__ -    Train | Loss: 2.80329
04/29/2021 21:12:51 - INFO - __main__ -    Train | Loss: 2.62925
04/29/2021 21:13:27 - INFO - __main__ -    Train | Loss: 2.48001
04/29/2021 21:14:03 - INFO - __main__ -    Train | Loss: 2.35092
04/29/2021 21:14:39 - INFO - __main__ -    Train | Loss: 2.23711
04/29/2021 21:15:16 - INFO - __main__ -    Train | Loss: 2.13611
04/29/2021 21:15:52 - INFO - __main__ -    Train | Loss: 2.04811
04/29/2021 21:16:28 - INFO - __main__ -    Train | Loss: 1.96758
04/29/2021 21:17:04 - INFO - __main__ -    Train | Loss: 1.89598
04/29/2021 21:17:06 - INFO - __main__ -    Train | Loss: 1.89369
04/29/2021 21:17:06 - INFO - __main__ -    
Epoch 02 / 10
04/29/2021 21:17:42 - INFO - __main__ -    Train | Loss: 0.63385
04/29/2021 21:18:18 - INFO - __main__ -    Train | Loss: 0.59835
04/29/2021 21:18:55 - INFO - __main__ -    Train | Loss: 0.58186
04/29/2021 21:19:31 - INFO - __main__ -    Train | Loss: 0.57159
04/29/2021 21:20:07 - INFO - __main__ -    Train | Loss: 0.55656
04/29/2021 21:20:44 - INFO - __main__ -    Train | Loss: 0.54259
04/29/2021 21:21:20 - INFO - __main__ -    Train | Loss: 0.53820
04/29/2021 21:21:57 - INFO - __main__ -    Train | Loss: 0.52670
04/29/2021 21:22:33 - INFO - __main__ -    Train | Loss: 0.51974
04/29/2021 21:23:09 - INFO - __main__ -    Train | Loss: 0.51342
04/29/2021 21:23:46 - INFO - __main__ -    Train | Loss: 0.50879
04/29/2021 21:24:22 - INFO - __main__ -    Train | Loss: 0.50656
04/29/2021 21:24:58 - INFO - __main__ -    Train | Loss: 0.50061
04/29/2021 21:25:35 - INFO - __main__ -    Train | Loss: 0.49677
04/29/2021 21:26:11 - INFO - __main__ -    Train | Loss: 0.49545
04/29/2021 21:26:47 - INFO - __main__ -    Train | Loss: 0.49254
04/29/2021 21:27:24 - INFO - __main__ -    Train | Loss: 0.49095
04/29/2021 21:28:00 - INFO - __main__ -    Train | Loss: 0.48686
04/29/2021 21:28:01 - INFO - __main__ -    Train | Loss: 0.48695
04/29/2021 21:28:01 - INFO - __main__ -    
Epoch 03 / 10
04/29/2021 21:28:38 - INFO - __main__ -    Train | Loss: 0.32085
04/29/2021 21:29:14 - INFO - __main__ -    Train | Loss: 0.30918
04/29/2021 21:29:51 - INFO - __main__ -    Train | Loss: 0.30265
04/29/2021 21:30:27 - INFO - __main__ -    Train | Loss: 0.29869
04/29/2021 21:31:04 - INFO - __main__ -    Train | Loss: 0.29197
04/29/2021 21:31:40 - INFO - __main__ -    Train | Loss: 0.28378
04/29/2021 21:32:16 - INFO - __main__ -    Train | Loss: 0.28237
04/29/2021 21:32:53 - INFO - __main__ -    Train | Loss: 0.28093
04/29/2021 21:33:29 - INFO - __main__ -    Train | Loss: 0.28140
04/29/2021 21:34:05 - INFO - __main__ -    Train | Loss: 0.28042
04/29/2021 21:34:42 - INFO - __main__ -    Train | Loss: 0.28052
04/29/2021 21:35:18 - INFO - __main__ -    Train | Loss: 0.27861
04/29/2021 21:35:55 - INFO - __main__ -    Train | Loss: 0.27712
04/29/2021 21:36:31 - INFO - __main__ -    Train | Loss: 0.27703
04/29/2021 21:37:07 - INFO - __main__ -    Train | Loss: 0.27434
04/29/2021 21:37:44 - INFO - __main__ -    Train | Loss: 0.27476
04/29/2021 21:38:20 - INFO - __main__ -    Train | Loss: 0.27348
04/29/2021 21:38:56 - INFO - __main__ -    Train | Loss: 0.27400
04/29/2021 21:38:58 - INFO - __main__ -    Train | Loss: 0.27380
04/29/2021 21:38:58 - INFO - __main__ -    
Epoch 04 / 10
04/29/2021 21:39:34 - INFO - __main__ -    Train | Loss: 0.16858
04/29/2021 21:40:11 - INFO - __main__ -    Train | Loss: 0.16323
04/29/2021 21:40:47 - INFO - __main__ -    Train | Loss: 0.16307
04/29/2021 21:41:23 - INFO - __main__ -    Train | Loss: 0.15672
04/29/2021 21:42:00 - INFO - __main__ -    Train | Loss: 0.15473
04/29/2021 21:42:36 - INFO - __main__ -    Train | Loss: 0.15537
