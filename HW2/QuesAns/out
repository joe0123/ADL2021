nohup: ignoring input
2021-04-29 10:19:09.023266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
04/29/2021 10:19:09 - INFO - __main__ -    Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Use FP16 precision: False

04/29/2021 10:19:10 - WARNING - datasets.builder -    Using custom data configuration default-39699345ead90bdd
0 tables [00:00, ? tables/s]                            loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/boewoei0123/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.5.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/boewoei0123/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.5.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /home/boewoei0123/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /home/boewoei0123/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d
loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /home/boewoei0123/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /home/boewoei0123/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/boewoei0123/.cache/huggingface/datasets/json/default-39699345ead90bdd/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...
Dataset json downloaded and prepared to /home/boewoei0123/.cache/huggingface/datasets/json/default-39699345ead90bdd/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.
#0:   0%|          | 0/7 [00:00<?, ?ba/s]
#1:   0%|          | 0/7 [00:00<?, ?ba/s][A

#2:   0%|          | 0/7 [00:00<?, ?ba/s][A[A


#3:   0%|          | 0/7 [00:00<?, ?ba/s][A[A[A#0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.15ba/s]
#1:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.15ba/s][A

#2:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.15ba/s][A[A


#3:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.18ba/s][A[A[A#0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.21ba/s]
#1:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.21ba/s][A

#2:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.22ba/s][A[A


#3:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.25ba/s][A[A[A#0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.26ba/s]
#1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.26ba/s][A

#2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.25ba/s][A[A


#3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.29ba/s][A[A[A#0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.26ba/s]
#1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.26ba/s][A

#2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.25ba/s][A[A


#3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.27ba/s][A[A[A#0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:03<00:01,  1.29ba/s]
#1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:03<00:01,  1.29ba/s][A

#2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:03<00:01,  1.29ba/s][A[A


#3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:03<00:01,  1.31ba/s][A[A[A#0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:04<00:00,  1.31ba/s]#0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.50ba/s]
#1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:04<00:00,  1.31ba/s][A#1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.51ba/s]

#2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:04<00:00,  1.32ba/s][A[A


#3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:04<00:00,  1.32ba/s][A[A[A

#2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.78ba/s][A[A#2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.50ba/s]


#3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.79ba/s][A[A[A#3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.51ba/s]



04/29/2021 10:19:26 - INFO - __main__ -    
******** Running training ********
04/29/2021 10:19:26 - INFO - __main__ -    Num examples = 43283
04/29/2021 10:19:26 - INFO - __main__ -    Num Epochs = 10
04/29/2021 10:19:26 - INFO - __main__ -    Instantaneous batch size per device = 8
04/29/2021 10:19:26 - INFO - __main__ -    Total train batch size (w/ parallel, distributed & accumulation) = 128
04/29/2021 10:19:26 - INFO - __main__ -    Gradient Accumulation steps = 16
04/29/2021 10:19:26 - INFO - __main__ -    Total optimization steps = 3390
04/29/2021 10:19:26 - INFO - __main__ -    
Epoch 01 / 10
04/29/2021 10:20:02 - INFO - __main__ -    Train | Loss: 5.99319
04/29/2021 10:20:38 - INFO - __main__ -    Train | Loss: 5.69450
04/29/2021 10:21:14 - INFO - __main__ -    Train | Loss: 5.21335
04/29/2021 10:21:50 - INFO - __main__ -    Train | Loss: 4.76921
04/29/2021 10:22:26 - INFO - __main__ -    Train | Loss: 4.29683
04/29/2021 10:23:02 - INFO - __main__ -    Train | Loss: 3.85879
04/29/2021 10:23:38 - INFO - __main__ -    Train | Loss: 3.52164
04/29/2021 10:24:15 - INFO - __main__ -    Train | Loss: 3.24227
04/29/2021 10:24:51 - INFO - __main__ -    Train | Loss: 3.00772
04/29/2021 10:25:27 - INFO - __main__ -    Train | Loss: 2.80329
04/29/2021 10:26:03 - INFO - __main__ -    Train | Loss: 2.62925
04/29/2021 10:26:39 - INFO - __main__ -    Train | Loss: 2.48001
04/29/2021 10:27:16 - INFO - __main__ -    Train | Loss: 2.35092
04/29/2021 10:27:52 - INFO - __main__ -    Train | Loss: 2.23711
04/29/2021 10:28:28 - INFO - __main__ -    Train | Loss: 2.13611
04/29/2021 10:29:05 - INFO - __main__ -    Train | Loss: 2.04811
04/29/2021 10:29:41 - INFO - __main__ -    Train | Loss: 1.96758
04/29/2021 10:30:17 - INFO - __main__ -    Train | Loss: 1.89598
04/29/2021 10:30:19 - INFO - __main__ -    Train | Loss: 1.89369
04/29/2021 10:30:19 - INFO - __main__ -    
Epoch 02 / 10
04/29/2021 10:30:55 - INFO - __main__ -    Train | Loss: 0.63385
04/29/2021 10:31:31 - INFO - __main__ -    Train | Loss: 0.59835
04/29/2021 10:32:08 - INFO - __main__ -    Train | Loss: 0.58186
04/29/2021 10:32:44 - INFO - __main__ -    Train | Loss: 0.57159
04/29/2021 10:33:21 - INFO - __main__ -    Train | Loss: 0.55656
04/29/2021 10:33:57 - INFO - __main__ -    Train | Loss: 0.54259
04/29/2021 10:34:33 - INFO - __main__ -    Train | Loss: 0.53820
04/29/2021 10:35:10 - INFO - __main__ -    Train | Loss: 0.52670
04/29/2021 10:35:46 - INFO - __main__ -    Train | Loss: 0.51974
04/29/2021 10:36:22 - INFO - __main__ -    Train | Loss: 0.51342
04/29/2021 10:36:59 - INFO - __main__ -    Train | Loss: 0.50879
04/29/2021 10:37:35 - INFO - __main__ -    Train | Loss: 0.50656
04/29/2021 10:38:11 - INFO - __main__ -    Train | Loss: 0.50061
04/29/2021 10:38:48 - INFO - __main__ -    Train | Loss: 0.49677
04/29/2021 10:39:24 - INFO - __main__ -    Train | Loss: 0.49545
04/29/2021 10:40:01 - INFO - __main__ -    Train | Loss: 0.49254
04/29/2021 10:40:37 - INFO - __main__ -    Train | Loss: 0.49095
04/29/2021 10:41:13 - INFO - __main__ -    Train | Loss: 0.48686
04/29/2021 10:41:15 - INFO - __main__ -    Train | Loss: 0.48695
04/29/2021 10:41:15 - INFO - __main__ -    
Epoch 03 / 10
04/29/2021 10:41:51 - INFO - __main__ -    Train | Loss: 0.32085
04/29/2021 10:42:27 - INFO - __main__ -    Train | Loss: 0.30918
04/29/2021 10:43:04 - INFO - __main__ -    Train | Loss: 0.30265
04/29/2021 10:43:40 - INFO - __main__ -    Train | Loss: 0.29869
04/29/2021 10:44:16 - INFO - __main__ -    Train | Loss: 0.29197
04/29/2021 10:44:53 - INFO - __main__ -    Train | Loss: 0.28378
04/29/2021 10:45:29 - INFO - __main__ -    Train | Loss: 0.28237
04/29/2021 10:46:06 - INFO - __main__ -    Train | Loss: 0.28093
04/29/2021 10:46:42 - INFO - __main__ -    Train | Loss: 0.28140
04/29/2021 10:47:18 - INFO - __main__ -    Train | Loss: 0.28042
04/29/2021 10:47:55 - INFO - __main__ -    Train | Loss: 0.28052
04/29/2021 10:48:31 - INFO - __main__ -    Train | Loss: 0.27861
04/29/2021 10:49:07 - INFO - __main__ -    Train | Loss: 0.27712
04/29/2021 10:49:44 - INFO - __main__ -    Train | Loss: 0.27703
04/29/2021 10:50:20 - INFO - __main__ -    Train | Loss: 0.27434
04/29/2021 10:50:57 - INFO - __main__ -    Train | Loss: 0.27476
04/29/2021 10:51:33 - INFO - __main__ -    Train | Loss: 0.27348
04/29/2021 10:52:09 - INFO - __main__ -    Train | Loss: 0.27400
04/29/2021 10:52:11 - INFO - __main__ -    Train | Loss: 0.27380
04/29/2021 10:52:11 - INFO - __main__ -    
Epoch 04 / 10
04/29/2021 10:52:47 - INFO - __main__ -    Train | Loss: 0.16858
04/29/2021 10:53:23 - INFO - __main__ -    Train | Loss: 0.16323
04/29/2021 10:54:00 - INFO - __main__ -    Train | Loss: 0.16307
04/29/2021 10:54:36 - INFO - __main__ -    Train | Loss: 0.15672
04/29/2021 10:55:12 - INFO - __main__ -    Train | Loss: 0.15473
04/29/2021 10:55:49 - INFO - __main__ -    Train | Loss: 0.15537
04/29/2021 10:56:25 - INFO - __main__ -    Train | Loss: 0.15396
04/29/2021 10:57:02 - INFO - __main__ -    Train | Loss: 0.15307
04/29/2021 10:57:38 - INFO - __main__ -    Train | Loss: 0.15341
04/29/2021 10:58:14 - INFO - __main__ -    Train | Loss: 0.15342
04/29/2021 10:58:51 - INFO - __main__ -    Train | Loss: 0.15168
04/29/2021 10:59:27 - INFO - __main__ -    Train | Loss: 0.15146
04/29/2021 11:00:03 - INFO - __main__ -    Train | Loss: 0.15256
04/29/2021 11:00:40 - INFO - __main__ -    Train | Loss: 0.15366
04/29/2021 11:01:16 - INFO - __main__ -    Train | Loss: 0.15288
04/29/2021 11:01:53 - INFO - __main__ -    Train | Loss: 0.15394
04/29/2021 11:02:29 - INFO - __main__ -    Train | Loss: 0.15454
04/29/2021 11:03:05 - INFO - __main__ -    Train | Loss: 0.15454
04/29/2021 11:03:07 - INFO - __main__ -    Train | Loss: 0.15445
04/29/2021 11:03:07 - INFO - __main__ -    
Epoch 05 / 10
04/29/2021 11:03:43 - INFO - __main__ -    Train | Loss: 0.10803
04/29/2021 11:04:19 - INFO - __main__ -    Train | Loss: 0.09725
04/29/2021 11:04:56 - INFO - __main__ -    Train | Loss: 0.09346
04/29/2021 11:05:32 - INFO - __main__ -    Train | Loss: 0.09302
04/29/2021 11:06:09 - INFO - __main__ -    Train | Loss: 0.09085
04/29/2021 11:06:45 - INFO - __main__ -    Train | Loss: 0.09051
04/29/2021 11:07:21 - INFO - __main__ -    Train | Loss: 0.09160
04/29/2021 11:07:58 - INFO - __main__ -    Train | Loss: 0.09153
04/29/2021 11:08:34 - INFO - __main__ -    Train | Loss: 0.09172
04/29/2021 11:09:11 - INFO - __main__ -    Train | Loss: 0.09160
04/29/2021 11:09:47 - INFO - __main__ -    Train | Loss: 0.08996
04/29/2021 11:10:23 - INFO - __main__ -    Train | Loss: 0.09172
04/29/2021 11:11:00 - INFO - __main__ -    Train | Loss: 0.09261
04/29/2021 11:11:36 - INFO - __main__ -    Train | Loss: 0.09287
04/29/2021 11:12:13 - INFO - __main__ -    Train | Loss: 0.09374
04/29/2021 11:12:49 - INFO - __main__ -    Train | Loss: 0.09569
04/29/2021 11:13:25 - INFO - __main__ -    Train | Loss: 0.09570
04/29/2021 11:14:02 - INFO - __main__ -    Train | Loss: 0.09577
04/29/2021 11:14:03 - INFO - __main__ -    Train | Loss: 0.09565
04/29/2021 11:14:03 - INFO - __main__ -    
Epoch 06 / 10
04/29/2021 11:14:39 - INFO - __main__ -    Train | Loss: 0.05869
04/29/2021 11:15:16 - INFO - __main__ -    Train | Loss: 0.05738
04/29/2021 11:15:52 - INFO - __main__ -    Train | Loss: 0.05644
04/29/2021 11:16:29 - INFO - __main__ -    Train | Loss: 0.05587
04/29/2021 11:17:05 - INFO - __main__ -    Train | Loss: 0.05625
04/29/2021 11:17:41 - INFO - __main__ -    Train | Loss: 0.05739
04/29/2021 11:18:18 - INFO - __main__ -    Train | Loss: 0.05780
04/29/2021 11:18:54 - INFO - __main__ -    Train | Loss: 0.05785
04/29/2021 11:19:30 - INFO - __main__ -    Train | Loss: 0.05803
04/29/2021 11:20:07 - INFO - __main__ -    Train | Loss: 0.05799
04/29/2021 11:20:43 - INFO - __main__ -    Train | Loss: 0.05795
04/29/2021 11:21:20 - INFO - __main__ -    Train | Loss: 0.05753
04/29/2021 11:21:56 - INFO - __main__ -    Train | Loss: 0.05736
04/29/2021 11:22:32 - INFO - __main__ -    Train | Loss: 0.05770
04/29/2021 11:23:09 - INFO - __main__ -    Train | Loss: 0.05809
04/29/2021 11:23:45 - INFO - __main__ -    Train | Loss: 0.05819
04/29/2021 11:24:21 - INFO - __main__ -    Train | Loss: 0.05830
04/29/2021 11:24:58 - INFO - __main__ -    Train | Loss: 0.05837
04/29/2021 11:24:59 - INFO - __main__ -    Train | Loss: 0.05845
04/29/2021 11:24:59 - INFO - __main__ -    
Epoch 07 / 10
04/29/2021 11:25:35 - INFO - __main__ -    Train | Loss: 0.04354
04/29/2021 11:26:12 - INFO - __main__ -    Train | Loss: 0.04522
04/29/2021 11:26:51 - INFO - __main__ -    Train | Loss: 0.04529
04/29/2021 11:27:31 - INFO - __main__ -    Train | Loss: 0.04404
04/29/2021 11:28:12 - INFO - __main__ -    Train | Loss: 0.04369
04/29/2021 11:28:53 - INFO - __main__ -    Train | Loss: 0.04271
04/29/2021 11:29:33 - INFO - __main__ -    Train | Loss: 0.04369
04/29/2021 11:30:14 - INFO - __main__ -    Train | Loss: 0.04233
04/29/2021 11:30:55 - INFO - __main__ -    Train | Loss: 0.04198
04/29/2021 11:31:35 - INFO - __main__ -    Train | Loss: 0.04194
04/29/2021 11:32:16 - INFO - __main__ -    Train | Loss: 0.04111
04/29/2021 11:32:57 - INFO - __main__ -    Train | Loss: 0.04113
04/29/2021 11:33:38 - INFO - __main__ -    Train | Loss: 0.04088
04/29/2021 11:34:18 - INFO - __main__ -    Train | Loss: 0.03993
04/29/2021 11:34:59 - INFO - __main__ -    Train | Loss: 0.04037
04/29/2021 11:35:40 - INFO - __main__ -    Train | Loss: 0.04012
04/29/2021 11:36:20 - INFO - __main__ -    Train | Loss: 0.03934
04/29/2021 11:37:01 - INFO - __main__ -    Train | Loss: 0.03980
04/29/2021 11:37:03 - INFO - __main__ -    Train | Loss: 0.03979
04/29/2021 11:37:03 - INFO - __main__ -    
Epoch 08 / 10
04/29/2021 11:37:43 - INFO - __main__ -    Train | Loss: 0.03393
04/29/2021 11:38:24 - INFO - __main__ -    Train | Loss: 0.03170
04/29/2021 11:39:05 - INFO - __main__ -    Train | Loss: 0.02947
04/29/2021 11:39:46 - INFO - __main__ -    Train | Loss: 0.02934
04/29/2021 11:40:27 - INFO - __main__ -    Train | Loss: 0.02906
04/29/2021 11:41:09 - INFO - __main__ -    Train | Loss: 0.03053
04/29/2021 11:41:49 - INFO - __main__ -    Train | Loss: 0.03006
04/29/2021 11:42:30 - INFO - __main__ -    Train | Loss: 0.02993
04/29/2021 11:43:11 - INFO - __main__ -    Train | Loss: 0.02974
04/29/2021 11:43:51 - INFO - __main__ -    Train | Loss: 0.02936
04/29/2021 11:44:32 - INFO - __main__ -    Train | Loss: 0.02903
04/29/2021 11:45:13 - INFO - __main__ -    Train | Loss: 0.02897
04/29/2021 11:45:54 - INFO - __main__ -    Train | Loss: 0.02870
04/29/2021 11:46:34 - INFO - __main__ -    Train | Loss: 0.02821
04/29/2021 11:47:15 - INFO - __main__ -    Train | Loss: 0.02842
04/29/2021 11:47:56 - INFO - __main__ -    Train | Loss: 0.02789
04/29/2021 11:48:36 - INFO - __main__ -    Train | Loss: 0.02757
04/29/2021 11:49:17 - INFO - __main__ -    Train | Loss: 0.02750
04/29/2021 11:49:19 - INFO - __main__ -    Train | Loss: 0.02751
04/29/2021 11:49:19 - INFO - __main__ -    
Epoch 09 / 10
04/29/2021 11:49:59 - INFO - __main__ -    Train | Loss: 0.01491
04/29/2021 11:50:40 - INFO - __main__ -    Train | Loss: 0.01769
04/29/2021 11:51:21 - INFO - __main__ -    Train | Loss: 0.01816
04/29/2021 11:52:01 - INFO - __main__ -    Train | Loss: 0.01774
04/29/2021 11:52:42 - INFO - __main__ -    Train | Loss: 0.01779
04/29/2021 11:53:23 - INFO - __main__ -    Train | Loss: 0.01804
04/29/2021 11:54:04 - INFO - __main__ -    Train | Loss: 0.01853
04/29/2021 11:54:44 - INFO - __main__ -    Train | Loss: 0.01875
04/29/2021 11:55:26 - INFO - __main__ -    Train | Loss: 0.01890
04/29/2021 11:56:07 - INFO - __main__ -    Train | Loss: 0.01946
04/29/2021 11:56:49 - INFO - __main__ -    Train | Loss: 0.01957
04/29/2021 11:57:30 - INFO - __main__ -    Train | Loss: 0.01989
04/29/2021 11:58:12 - INFO - __main__ -    Train | Loss: 0.01929
04/29/2021 11:58:54 - INFO - __main__ -    Train | Loss: 0.01900
04/29/2021 11:59:35 - INFO - __main__ -    Train | Loss: 0.01870
04/29/2021 12:00:17 - INFO - __main__ -    Train | Loss: 0.01861
04/29/2021 12:00:58 - INFO - __main__ -    Train | Loss: 0.01843
04/29/2021 12:01:40 - INFO - __main__ -    Train | Loss: 0.01823
04/29/2021 12:01:41 - INFO - __main__ -    Train | Loss: 0.01821
04/29/2021 12:01:41 - INFO - __main__ -    
Epoch 10 / 10
04/29/2021 12:02:23 - INFO - __main__ -    Train | Loss: 0.01408
04/29/2021 12:03:05 - INFO - __main__ -    Train | Loss: 0.01520
04/29/2021 12:03:46 - INFO - __main__ -    Train | Loss: 0.01605
04/29/2021 12:04:27 - INFO - __main__ -    Train | Loss: 0.01634
04/29/2021 12:05:08 - INFO - __main__ -    Train | Loss: 0.01549
04/29/2021 12:05:48 - INFO - __main__ -    Train | Loss: 0.01439
04/29/2021 12:06:29 - INFO - __main__ -    Train | Loss: 0.01429
04/29/2021 12:07:10 - INFO - __main__ -    Train | Loss: 0.01407
04/29/2021 12:07:50 - INFO - __main__ -    Train | Loss: 0.01381
04/29/2021 12:08:31 - INFO - __main__ -    Train | Loss: 0.01361
04/29/2021 12:09:12 - INFO - __main__ -    Train | Loss: 0.01367
04/29/2021 12:09:52 - INFO - __main__ -    Train | Loss: 0.01363
04/29/2021 12:10:33 - INFO - __main__ -    Train | Loss: 0.01374
04/29/2021 12:11:14 - INFO - __main__ -    Train | Loss: 0.01354
04/29/2021 12:11:55 - INFO - __main__ -    Train | Loss: 0.01340
04/29/2021 12:12:35 - INFO - __main__ -    Train | Loss: 0.01355
04/29/2021 12:13:16 - INFO - __main__ -    Train | Loss: 0.01348
04/29/2021 12:13:57 - INFO - __main__ -    Train | Loss: 0.01362
04/29/2021 12:13:58 - INFO - __main__ -    Train | Loss: 0.01362
