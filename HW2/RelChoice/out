nohup: ignoring input
2021-05-02 04:19:29.826085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
05/02/2021 04:19:31 - INFO - __main__ -    Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Use FP16 precision: False

loading configuration file https://huggingface.co/hfl/chinese-xlnet-base/resolve/main/config.json from cache at /home/joehuang/.cache/huggingface/transformers/8b32d82613732358ebd14a0aec836e0092922cb81fe944dba9d75ffdbdb5767f.8f39eb26d38bdc8cf5a64796e556f2675f19d84ee1952698e8ddf39372c16e86
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "relu",
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "output_past": true,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "transformers_version": "4.5.0",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading configuration file https://huggingface.co/hfl/chinese-xlnet-base/resolve/main/config.json from cache at /home/joehuang/.cache/huggingface/transformers/8b32d82613732358ebd14a0aec836e0092922cb81fe944dba9d75ffdbdb5767f.8f39eb26d38bdc8cf5a64796e556f2675f19d84ee1952698e8ddf39372c16e86
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "relu",
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 12,
  "n_layer": 12,
  "output_past": true,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "transformers_version": "4.5.0",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading file https://huggingface.co/hfl/chinese-xlnet-base/resolve/main/spiece.model from cache at /home/joehuang/.cache/huggingface/transformers/9345b8cd17628eee3d057985cbcdb87426a05dac03927c0883450fbb1a569f9f.6ea411ea38650fd3ef16b0358032669ef47b3189b437d8384e381ddb8e3bf1fc
loading file https://huggingface.co/hfl/chinese-xlnet-base/resolve/main/tokenizer.json from cache at /home/joehuang/.cache/huggingface/transformers/5f75bb0d377fac1e48238ff8642202b903c8da5496939fd658aea541a58fc7d8.f2920ee526ef61b738ba712d8d8d1bb2b53ba1a553ab15544f980b275aa3a241
loading file https://huggingface.co/hfl/chinese-xlnet-base/resolve/main/added_tokens.json from cache at /home/joehuang/.cache/huggingface/transformers/dcccb00fccb20365fb3ef599009b627e991d368e5c15c944020146317a841eb5.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b
loading file https://huggingface.co/hfl/chinese-xlnet-base/resolve/main/special_tokens_map.json from cache at /home/joehuang/.cache/huggingface/transformers/671aec90d8845d3e532547d77ba89aec76b417df8249e1541f6905df54077b32.d116fe7accce3fec558ec1ac6bb9c3a2373d0406d6407a0786c77194e7912f44
loading file https://huggingface.co/hfl/chinese-xlnet-base/resolve/main/tokenizer_config.json from cache at /home/joehuang/.cache/huggingface/transformers/cf826080d903bfa3d003b82a3cd341976ec4826f627573a24fea2eafaa5aaf96.d23f50bbddc3fb34db5a76d47fa9bdd5d75bf4201ad2d49abbcca25629b3e562
05/02/2021 04:19:36 - INFO - __main__ -    Saving tokenizer to ./saved/0502-0419/tokenizer...
tokenizer config file saved in ./saved/0502-0419/tokenizer_config.json
Special tokens file saved in ./saved/0502-0419/special_tokens_map.json
loading weights file https://huggingface.co/hfl/chinese-xlnet-base/resolve/main/pytorch_model.bin from cache at /home/joehuang/.cache/huggingface/transformers/4155c4f0f96b03bcfa202eac1e1aedb5e9786f4a857d02e0b657b65379dab8bd.72b0ec262ea4c39f29e3639455dfab61b80c6598755289696f03721bb705efb4
Some weights of the model checkpoint at hfl/chinese-xlnet-base were not used when initializing XLNetForMultipleChoice: ['lm_loss.weight', 'lm_loss.bias']
- This IS expected if you are initializing XLNetForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForMultipleChoice were not initialized from the model checkpoint at hfl/chinese-xlnet-base and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/02/2021 04:19:41 - WARNING - datasets.builder -    Using custom data configuration default-34f792cd9b376eed
0 tables [00:00, ? tables/s]1 tables [00:00,  4.94 tables/s]                                Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/joehuang/.cache/huggingface/datasets/json/default-34f792cd9b376eed/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...
Dataset json downloaded and prepared to /home/joehuang/.cache/huggingface/datasets/json/default-34f792cd9b376eed/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.
#0:   0%|          | 0/7 [00:00<?, ?ba/s]
#1:   0%|          | 0/7 [00:00<?, ?ba/s][A

#2:   0%|          | 0/7 [00:00<?, ?ba/s][A[A


#3:   0%|          | 0/7 [00:00<?, ?ba/s][A[A[A#0:  14%|█▍        | 1/7 [00:03<00:21,  3.66s/ba]
#1:  14%|█▍        | 1/7 [00:03<00:20,  3.46s/ba][A

#2:  14%|█▍        | 1/7 [00:03<00:20,  3.49s/ba][A[A


#3:  14%|█▍        | 1/7 [00:03<00:21,  3.63s/ba][A[A[A#0:  29%|██▊       | 2/7 [00:07<00:17,  3.59s/ba]
#1:  29%|██▊       | 2/7 [00:07<00:17,  3.50s/ba][A

#2:  29%|██▊       | 2/7 [00:06<00:17,  3.43s/ba][A[A


#3:  29%|██▊       | 2/7 [00:07<00:18,  3.63s/ba][A[A[A#0:  43%|████▎     | 3/7 [00:10<00:14,  3.54s/ba]
#1:  43%|████▎     | 3/7 [00:10<00:13,  3.45s/ba][A

#2:  43%|████▎     | 3/7 [00:10<00:13,  3.41s/ba][A[A


#3:  43%|████▎     | 3/7 [00:10<00:14,  3.61s/ba][A[A[A#0:  57%|█████▋    | 4/7 [00:13<00:10,  3.47s/ba]
#1:  57%|█████▋    | 4/7 [00:13<00:10,  3.41s/ba][A

#2:  57%|█████▋    | 4/7 [00:13<00:10,  3.38s/ba][A[A


#3:  57%|█████▋    | 4/7 [00:14<00:10,  3.58s/ba][A[A[A#0:  71%|███████▏  | 5/7 [00:17<00:06,  3.45s/ba]
#1:  71%|███████▏  | 5/7 [00:17<00:06,  3.41s/ba][A

#2:  71%|███████▏  | 5/7 [00:16<00:06,  3.42s/ba][A[A


#3:  71%|███████▏  | 5/7 [00:17<00:07,  3.55s/ba][A[A[A#0:  86%|████████▌ | 6/7 [00:20<00:03,  3.45s/ba]
#1:  86%|████████▌ | 6/7 [00:20<00:03,  3.42s/ba][A#0: 100%|██████████| 7/7 [00:21<00:00,  2.59s/ba]#0: 100%|██████████| 7/7 [00:21<00:00,  3.04s/ba]
#1: 100%|██████████| 7/7 [00:20<00:00,  2.52s/ba][A#1: 100%|██████████| 7/7 [00:20<00:00,  3.00s/ba]

#2:  86%|████████▌ | 6/7 [00:20<00:03,  3.40s/ba][A[A

#2: 100%|██████████| 7/7 [00:20<00:00,  2.51s/ba][A[A#2: 100%|██████████| 7/7 [00:20<00:00,  2.97s/ba]


#3:  86%|████████▌ | 6/7 [00:21<00:03,  3.52s/ba][A[A[A


#3: 100%|██████████| 7/7 [00:21<00:00,  2.62s/ba][A[A[A#3: 100%|██████████| 7/7 [00:21<00:00,  3.11s/ba]



05/02/2021 04:20:10 - INFO - __main__ -    
******** Running training ********
05/02/2021 04:20:10 - INFO - __main__ -    Num train examples = 24410
05/02/2021 04:20:10 - INFO - __main__ -    Num Epochs = 3
05/02/2021 04:20:10 - INFO - __main__ -    Instantaneous batch size per device = 4
05/02/2021 04:20:10 - INFO - __main__ -    Total train batch size (w/ parallel, distributed & accumulation) = 64
05/02/2021 04:20:10 - INFO - __main__ -    Instantaneous steps per epoch = 6103
05/02/2021 04:20:10 - INFO - __main__ -    Update steps per epoch = 382
05/02/2021 04:20:10 - INFO - __main__ -    Total update steps = 1146
05/02/2021 04:20:10 - INFO - __main__ -    
Epoch 01 / 03
05/02/2021 04:27:38 - INFO - __main__ -    Train | Loss: 1.58689
05/02/2021 04:35:07 - INFO - __main__ -    Train | Loss: 1.16485
05/02/2021 04:42:37 - INFO - __main__ -    Train | Loss: 0.85961
05/02/2021 04:50:07 - INFO - __main__ -    Train | Loss: 0.69425
05/02/2021 04:57:36 - INFO - __main__ -    Train | Loss: 0.58889
05/02/2021 05:05:06 - INFO - __main__ -    Train | Loss: 0.51582
05/02/2021 05:12:35 - INFO - __main__ -    Train | Loss: 0.46229
05/02/2021 05:20:07 - INFO - __main__ -    Train | Loss: 0.42269
05/02/2021 05:27:40 - INFO - __main__ -    Train | Loss: 0.39175
05/02/2021 05:35:12 - INFO - __main__ -    Train | Loss: 0.36476
05/02/2021 05:42:43 - INFO - __main__ -    Train | Loss: 0.34245
05/02/2021 05:50:14 - INFO - __main__ -    Train | Loss: 0.32380
05/02/2021 05:51:47 - INFO - __main__ -    Train | Loss: 0.32038
05/02/2021 05:51:48 - INFO - __main__ -    
Epoch 02 / 03
05/02/2021 05:59:17 - INFO - __main__ -    Train | Loss: 0.10935
05/02/2021 06:06:47 - INFO - __main__ -    Train | Loss: 0.09854
05/02/2021 06:14:15 - INFO - __main__ -    Train | Loss: 0.09627
05/02/2021 06:21:42 - INFO - __main__ -    Train | Loss: 0.09263
05/02/2021 06:29:11 - INFO - __main__ -    Train | Loss: 0.09415
05/02/2021 06:36:39 - INFO - __main__ -    Train | Loss: 0.09192
05/02/2021 06:44:07 - INFO - __main__ -    Train | Loss: 0.09258
05/02/2021 06:51:34 - INFO - __main__ -    Train | Loss: 0.09163
05/02/2021 06:59:03 - INFO - __main__ -    Train | Loss: 0.09135
05/02/2021 07:06:33 - INFO - __main__ -    Train | Loss: 0.08960
05/02/2021 07:14:04 - INFO - __main__ -    Train | Loss: 0.08880
05/02/2021 07:21:34 - INFO - __main__ -    Train | Loss: 0.08820
05/02/2021 07:23:06 - INFO - __main__ -    Train | Loss: 0.08811
05/02/2021 07:23:06 - INFO - __main__ -    
Epoch 03 / 03
05/02/2021 07:30:34 - INFO - __main__ -    Train | Loss: 0.05638
05/02/2021 07:38:03 - INFO - __main__ -    Train | Loss: 0.05031
05/02/2021 07:45:33 - INFO - __main__ -    Train | Loss: 0.05359
05/02/2021 07:53:01 - INFO - __main__ -    Train | Loss: 0.05436
05/02/2021 08:00:28 - INFO - __main__ -    Train | Loss: 0.05506
05/02/2021 08:07:55 - INFO - __main__ -    Train | Loss: 0.05660
05/02/2021 08:15:25 - INFO - __main__ -    Train | Loss: 0.05623
05/02/2021 08:22:55 - INFO - __main__ -    Train | Loss: 0.05611
05/02/2021 08:30:24 - INFO - __main__ -    Train | Loss: 0.05626
05/02/2021 08:37:53 - INFO - __main__ -    Train | Loss: 0.05615
05/02/2021 08:45:24 - INFO - __main__ -    Train | Loss: 0.05549
05/02/2021 08:52:55 - INFO - __main__ -    Train | Loss: 0.05503
05/02/2021 08:54:28 - INFO - __main__ -    Train | Loss: 0.05509
Configuration saved in ./saved/0502-0419/config.json
Model weights saved in ./saved/0502-0419/pytorch_model.bin
05/02/2021 08:54:28 - INFO - __main__ -    Saving config and model to ./saved/0502-0419...
2021-05-02 08:54:30.949379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
05/02/2021 08:54:32 - INFO - __main__ -    Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Use FP16 precision: False

loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/joehuang/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.5.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/joehuang/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.5.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /home/joehuang/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /home/joehuang/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d
loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /home/joehuang/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
05/02/2021 08:54:37 - INFO - __main__ -    Saving tokenizer to ./saved/0502-0854/tokenizer...
tokenizer config file saved in ./saved/0502-0854/tokenizer_config.json
Special tokens file saved in ./saved/0502-0854/special_tokens_map.json
loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /home/joehuang/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508
Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/02/2021 08:54:43 - WARNING - datasets.builder -    Using custom data configuration default-34f792cd9b376eed
05/02/2021 08:54:43 - WARNING - datasets.builder -    Reusing dataset json (/home/joehuang/.cache/huggingface/datasets/json/default-34f792cd9b376eed/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)
#0:   0%|          | 0/7 [00:00<?, ?ba/s]
#1:   0%|          | 0/7 [00:00<?, ?ba/s][A

#2:   0%|          | 0/7 [00:00<?, ?ba/s][A[A


#3:   0%|          | 0/7 [00:00<?, ?ba/s][A[A[A#0:  14%|█▍        | 1/7 [00:06<00:38,  6.41s/ba]
#1:  14%|█▍        | 1/7 [00:06<00:40,  6.74s/ba][A

#2:  14%|█▍        | 1/7 [00:06<00:38,  6.34s/ba][A[A


#3:  14%|█▍        | 1/7 [00:06<00:36,  6.11s/ba][A[A[A#0:  29%|██▊       | 2/7 [00:12<00:31,  6.33s/ba]
#1:  29%|██▊       | 2/7 [00:13<00:32,  6.60s/ba][A

#2:  29%|██▊       | 2/7 [00:12<00:31,  6.29s/ba][A[A


#3:  29%|██▊       | 2/7 [00:12<00:30,  6.15s/ba][A[A[A#0:  43%|████▎     | 3/7 [00:18<00:25,  6.26s/ba]
#1:  43%|████▎     | 3/7 [00:19<00:25,  6.47s/ba][A

#2:  43%|████▎     | 3/7 [00:18<00:25,  6.26s/ba][A[A


#3:  43%|████▎     | 3/7 [00:18<00:24,  6.24s/ba][A[A[A#0:  57%|█████▋    | 4/7 [00:24<00:18,  6.19s/ba]
#1:  57%|█████▋    | 4/7 [00:25<00:19,  6.35s/ba][A

#2:  57%|█████▋    | 4/7 [00:24<00:18,  6.22s/ba][A[A


#3:  57%|█████▋    | 4/7 [00:25<00:18,  6.25s/ba][A[A[A#0:  71%|███████▏  | 5/7 [00:30<00:12,  6.17s/ba]
#1:  71%|███████▏  | 5/7 [00:31<00:12,  6.27s/ba][A

#2:  71%|███████▏  | 5/7 [00:31<00:12,  6.23s/ba][A[A


#3:  71%|███████▏  | 5/7 [00:31<00:12,  6.28s/ba][A[A[A#0:  86%|████████▌ | 6/7 [00:36<00:06,  6.13s/ba]#0: 100%|██████████| 7/7 [00:37<00:00,  4.51s/ba]#0: 100%|██████████| 7/7 [00:37<00:00,  5.37s/ba]
#1:  86%|████████▌ | 6/7 [00:37<00:06,  6.16s/ba][A
#1: 100%|██████████| 7/7 [00:37<00:00,  4.51s/ba][A#1: 100%|██████████| 7/7 [00:37<00:00,  5.41s/ba]

#2:  86%|████████▌ | 6/7 [00:37<00:06,  6.23s/ba][A[A

#2: 100%|██████████| 7/7 [00:37<00:00,  4.55s/ba][A[A#2: 100%|██████████| 7/7 [00:37<00:00,  5.42s/ba]


#3:  86%|████████▌ | 6/7 [00:37<00:06,  6.29s/ba][A[A[A


#3: 100%|██████████| 7/7 [00:38<00:00,  4.64s/ba][A[A[A#3: 100%|██████████| 7/7 [00:38<00:00,  5.50s/ba]



05/02/2021 08:55:29 - INFO - __main__ -    
******** Running training ********
05/02/2021 08:55:29 - INFO - __main__ -    Num train examples = 24410
05/02/2021 08:55:29 - INFO - __main__ -    Num Epochs = 3
05/02/2021 08:55:29 - INFO - __main__ -    Instantaneous batch size per device = 4
05/02/2021 08:55:29 - INFO - __main__ -    Total train batch size (w/ parallel, distributed & accumulation) = 64
05/02/2021 08:55:29 - INFO - __main__ -    Instantaneous steps per epoch = 6103
05/02/2021 08:55:29 - INFO - __main__ -    Update steps per epoch = 382
05/02/2021 08:55:29 - INFO - __main__ -    Total update steps = 1146
05/02/2021 08:55:29 - INFO - __main__ -    
Epoch 01 / 03
05/02/2021 08:58:13 - INFO - __main__ -    Train | Loss: 1.37927
05/02/2021 09:00:57 - INFO - __main__ -    Train | Loss: 0.96988
05/02/2021 09:03:41 - INFO - __main__ -    Train | Loss: 0.74793
