nohup: ignoring input
2021-05-02 15:43:11.012082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
05/02/2021 15:43:12 - INFO - __main__ -    Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Use FP16 precision: False

loading configuration file saved/q4/config.json
Model config BertConfig {
  "_name_or_path": "bert-base-chinese",
  "architectures": [
    "BertForMultipleChoice"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.5.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/joehuang/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "position_embedding_type": "absolute",
  "transformers_version": "4.5.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /home/joehuang/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /home/joehuang/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d
loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /home/joehuang/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
05/02/2021 15:43:16 - INFO - __main__ -    Saving tokenizer to ./saved/0502-1543/tokenizer...
tokenizer config file saved in ./saved/0502-1543/tokenizer_config.json
Special tokens file saved in ./saved/0502-1543/special_tokens_map.json
05/02/2021 15:43:17 - INFO - __main__ -    Training new model from scratch
05/02/2021 15:43:20 - WARNING - datasets.builder -    Using custom data configuration default-0cf9cf5d8c826c55
0 tables [00:00, ? tables/s]1 tables [00:00,  4.99 tables/s]                                Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/joehuang/.cache/huggingface/datasets/json/default-0cf9cf5d8c826c55/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...
Dataset json downloaded and prepared to /home/joehuang/.cache/huggingface/datasets/json/default-0cf9cf5d8c826c55/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.
#0:   0%|          | 0/7 [00:00<?, ?ba/s]
#1:   0%|          | 0/7 [00:00<?, ?ba/s][A

#2:   0%|          | 0/7 [00:00<?, ?ba/s][A[A


#3:   0%|          | 0/7 [00:00<?, ?ba/s][A[A[A#0:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  6.38s/ba]
#1:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  6.37s/ba][A

#2:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  6.43s/ba][A[A


#3:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  6.50s/ba][A[A[A#0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  6.36s/ba]
#1:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  6.38s/ba][A

#2:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:13<00:32,  6.49s/ba][A[A


#3:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:13<00:32,  6.52s/ba][A[A[A#0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  6.39s/ba]
#1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  6.40s/ba][A

#2:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  6.44s/ba][A[A


#3:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:26,  6.52s/ba][A[A[A#0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:18,  6.33s/ba]
#1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:19,  6.38s/ba][A

#2:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:19,  6.44s/ba][A[A


#3:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:19,  6.41s/ba][A[A[A#0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  6.29s/ba]
#1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  6.35s/ba][A

#2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  6.36s/ba][A[A


#3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  6.30s/ba][A[A[A#0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  6.22s/ba]#0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  4.55s/ba]#0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.46s/ba]
#1:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:38<00:06,  6.30s/ba][A

#2:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  6.22s/ba][A[A
#1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  4.60s/ba][A#1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.52s/ba]

#2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  4.55s/ba][A[A#2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.51s/ba]


#3:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  6.23s/ba][A[A[A


#3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  4.56s/ba][A[A[A#3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.50s/ba]



05/02/2021 15:44:06 - INFO - __main__ -    
******** Running training ********
05/02/2021 15:44:06 - INFO - __main__ -    Num train examples = 24410
05/02/2021 15:44:06 - INFO - __main__ -    Num Epochs = 5
05/02/2021 15:44:06 - INFO - __main__ -    Instantaneous batch size per device = 4
05/02/2021 15:44:06 - INFO - __main__ -    Total train batch size (w/ parallel, distributed & accumulation) = 64
05/02/2021 15:44:06 - INFO - __main__ -    Instantaneous steps per epoch = 6103
05/02/2021 15:44:06 - INFO - __main__ -    Update steps per epoch = 382
05/02/2021 15:44:06 - INFO - __main__ -    Total update steps = 1910
05/02/2021 15:44:06 - INFO - __main__ -    
Epoch 01 / 05
05/02/2021 15:46:50 - INFO - __main__ -    Train | Loss: 1.58115
05/02/2021 15:49:34 - INFO - __main__ -    Train | Loss: 1.43945
05/02/2021 15:52:18 - INFO - __main__ -    Train | Loss: 1.33830
